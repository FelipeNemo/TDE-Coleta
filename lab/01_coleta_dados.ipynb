{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a94f7f45",
   "metadata": {},
   "source": [
    "# Coleta de Dados da API da C√¢mara dos Deputados\n",
    "\n",
    "Este notebook realiza a coleta de dados da API da C√¢mara dos Deputados para an√°lise de:\n",
    "\n",
    "1. Presen√ßa dos deputados\n",
    "2. Participa√ß√£o em vota√ß√µes\n",
    "3. Compara√ß√£o com regime CLT\n",
    "4. An√°lise de remunera√ß√£o efetiva\n",
    "5. Compara√ß√£o com trabalhadores civis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7519ee55-1c61-4253-adcc-d8f81931af16",
   "metadata": {},
   "source": [
    "# üìò coleta_dados\n",
    "### **Objetivo:**\n",
    "- Coletar lista de deputados e seus eventos via API da C√¢mara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fe310c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import date\n",
    "from pathlib import Path\n",
    "\n",
    "# Configura√ß√£o b√°sica\n",
    "BASE_URL = \"https://dadosabertos.camara.leg.br/api/v2\"\n",
    "DATA_DIR = Path(\"../data/processed\")\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "105e9545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o auxiliar simples para requests\n",
    "def get_dados(endpoint, params=None):\n",
    "    r = requests.get(f\"{BASE_URL}/{endpoint}\", params=params)\n",
    "    r.raise_for_status()\n",
    "    return r.json()[\"dados\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48a2010",
   "metadata": {},
   "source": [
    "# 1. Deputados e Presen√ßas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0062f4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coleta lista de deputados\n",
    "params = {\"ordem\": \"ASC\", \"ordenarPor\": \"nome\"}\n",
    "deputados = pd.DataFrame(get_dados(\"deputados\", params))\n",
    "deputados.to_json(DATA_DIR / \"deputados.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb403cc",
   "metadata": {},
   "source": [
    "# 2. Eventos e Sess√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed6880a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coleta eventos (sess√µes deliberativas)\n",
    "from datetime import date\n",
    "params = {\n",
    "    \"dataInicio\": \"2020-01-01\",\n",
    "    \"dataFim\": str(date.today()),\n",
    "    \"codTipoEvento\": \"1,2\",  # Sess√µes deliberativas ordin√°rias e extraordin√°rias\n",
    "    \"ordem\": \"ASC\",\n",
    "    \"ordenarPor\": \"dataHoraInicio\"\n",
    "}\n",
    "eventos = pd.DataFrame(get_dados(\"eventos\", params))\n",
    "eventos.to_csv(DATA_DIR / \"eventos_sessoes.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65600331",
   "metadata": {},
   "source": [
    "# 3. Vota√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f421d9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Meses: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 70/70 [08:41<00:00,  7.46s/it]\n",
      "\n"
     ]
    },
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mEmptyDataError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 106\u001b[39m\n\u001b[32m    103\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[32m    105\u001b[39m \u001b[38;5;66;03m# ===== Execu√ß√£o =====\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m votacoes = \u001b[43mcoletar_votacoes_resumivel\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m2020-01-01\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoday\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m votos_df = coletar_votos_resumivel(votacoes, itens=\u001b[32m200\u001b[39m)\n\u001b[32m    108\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mOK:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(votacoes), \u001b[33m\"\u001b[39m\u001b[33mvota√ß√µes;\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(votos_df), \u001b[33m\"\u001b[39m\u001b[33mvotos.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 70\u001b[39m, in \u001b[36mcoletar_votacoes_resumivel\u001b[39m\u001b[34m(di, df, itens)\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# consolida (r√°pido e idempotente)\u001b[39;00m\n\u001b[32m     69\u001b[39m vot_files = \u001b[38;5;28msorted\u001b[39m(DATA_DIR.glob(\u001b[33m\"\u001b[39m\u001b[33mvotacoes_20*.csv\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvot_files\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m vot_files \u001b[38;5;28;01melse\u001b[39;00m pd.DataFrame()\n\u001b[32m     71\u001b[39m df.to_csv(DATA_DIR / \u001b[33m\"\u001b[39m\u001b[33mvotacoes.csv\u001b[39m\u001b[33m\"\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tde/lib/python3.11/site-packages/pandas/core/reshape/concat.py:382\u001b[39m, in \u001b[36mconcat\u001b[39m\u001b[34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[39m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[32m    380\u001b[39m     copy = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m382\u001b[39m op = \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m op.get_result()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tde/lib/python3.11/site-packages/pandas/core/reshape/concat.py:445\u001b[39m, in \u001b[36m_Concatenator.__init__\u001b[39m\u001b[34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[39m\n\u001b[32m    442\u001b[39m \u001b[38;5;28mself\u001b[39m.verify_integrity = verify_integrity\n\u001b[32m    443\u001b[39m \u001b[38;5;28mself\u001b[39m.copy = copy\n\u001b[32m--> \u001b[39m\u001b[32m445\u001b[39m objs, keys = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_clean_keys_and_objs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[32m    448\u001b[39m ndims = \u001b[38;5;28mself\u001b[39m._get_ndims(objs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tde/lib/python3.11/site-packages/pandas/core/reshape/concat.py:504\u001b[39m, in \u001b[36m_Concatenator._clean_keys_and_objs\u001b[39m\u001b[34m(self, objs, keys)\u001b[39m\n\u001b[32m    502\u001b[39m     objs_list = [objs[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m keys]\n\u001b[32m    503\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m504\u001b[39m     objs_list = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) == \u001b[32m0\u001b[39m:\n\u001b[32m    507\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo objects to concatenate\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 70\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# consolida (r√°pido e idempotente)\u001b[39;00m\n\u001b[32m     69\u001b[39m vot_files = \u001b[38;5;28msorted\u001b[39m(DATA_DIR.glob(\u001b[33m\"\u001b[39m\u001b[33mvotacoes_20*.csv\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m df = pd.concat((\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m vot_files), ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m vot_files \u001b[38;5;28;01melse\u001b[39;00m pd.DataFrame()\n\u001b[32m     71\u001b[39m df.to_csv(DATA_DIR / \u001b[33m\"\u001b[39m\u001b[33mvotacoes.csv\u001b[39m\u001b[33m\"\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tde/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tde/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tde/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tde/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1898\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1895\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m   1897\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1898\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1899\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1900\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tde/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:93\u001b[39m, in \u001b[36mCParserWrapper.__init__\u001b[39m\u001b[34m(self, src, **kwds)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[33m\"\u001b[39m\u001b[33mdtype_backend\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     91\u001b[39m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[32m     92\u001b[39m     import_optional_dependency(\u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[38;5;28mself\u001b[39m._reader = \u001b[43mparsers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28mself\u001b[39m.unnamed_cols = \u001b[38;5;28mself\u001b[39m._reader.unnamed_cols\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:581\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.__cinit__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mEmptyDataError\u001b[39m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "# Coleta vota√ß√µes (2020-hoje) e salva mensalmente com resume autom√°tico.\n",
    "import time, math, os, json, requests, pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import date\n",
    "from tqdm import tqdm\n",
    "from urllib3.util.retry import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "\n",
    "BASE = \"https://dadosabertos.camara.leg.br/api/v2\"\n",
    "DATA_DIR = Path(\"data/processed\"); DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "HEADERS = {\"Accept\":\"application/json\",\"User-Agent\":\"PUCRS-TDE/1.0\"}\n",
    "\n",
    "def make_session():\n",
    "    s = requests.Session()\n",
    "    retry = Retry(\n",
    "        total=5, connect=5, read=5, backoff_factor=1.2,\n",
    "        status_forcelist=[400, 429, 500, 502, 503, 504],\n",
    "        allowed_methods=[\"GET\"]\n",
    "    )\n",
    "    s.mount(\"https://\", HTTPAdapter(max_retries=retry))\n",
    "    s.headers.update(HEADERS)\n",
    "    return s\n",
    "\n",
    "def meses(start=\"2020-01-01\", end=str(date.today())):\n",
    "    s, e = pd.to_datetime(start), pd.to_datetime(end)\n",
    "    for m0 in pd.date_range(s, e, freq=\"MS\"):\n",
    "        m1 = (m0 + pd.offsets.MonthEnd(1))\n",
    "        if m1 > e: m1 = e\n",
    "        yield m0.strftime(\"%Y-%m\"), m0.date().isoformat(), m1.date().isoformat()\n",
    "\n",
    "def fetch_page(sess, endpoint, params):\n",
    "    # timeout=(connect, read) evita pendurar indefinidamente\n",
    "    r = sess.get(f\"{BASE}/{endpoint}\", params=params, timeout=(8, 40))\n",
    "    if r.status_code == 400 and \"ordenarPor\" in params:\n",
    "        # fallback sem ordenarPor quando d√° 400\n",
    "        p2 = dict(params); p2.pop(\"ordenarPor\", None)\n",
    "        r = sess.get(f\"{BASE}/{endpoint}\", params=p2, timeout=(8, 40))\n",
    "    r.raise_for_status()\n",
    "    j = r.json()\n",
    "    return j.get(\"dados\", [])\n",
    "\n",
    "def coletar_votacoes_resumivel(di=\"2020-01-01\", df=str(date.today()), itens=200):\n",
    "    sess = make_session()\n",
    "    for tag, ini, fim in tqdm(list(meses(di, df)), desc=\"Meses\"):\n",
    "        arq_mes = DATA_DIR / f\"votacoes_{tag}.csv\"\n",
    "        if arq_mes.exists() and arq_mes.stat().st_size > 0:\n",
    "            continue  # j√° coletado\n",
    "        rows, pag = [], 1\n",
    "        try:\n",
    "            while True:\n",
    "                params = {\n",
    "                    \"dataInicio\": ini, \"dataFim\": fim,\n",
    "                    \"pagina\": pag, \"itens\": itens, \"ordem\": \"ASC\",\n",
    "                    \"ordenarPor\": \"dataHoraRegistro\"\n",
    "                }\n",
    "                dados = fetch_page(sess, \"votacoes\", params)\n",
    "                if not dados: break\n",
    "                rows.extend(dados)\n",
    "                if len(dados) < itens: break\n",
    "                pag += 1\n",
    "                time.sleep(0.05)\n",
    "        except KeyboardInterrupt:\n",
    "            # salva parcial para n√£o perder o m√™s\n",
    "            if rows:\n",
    "                pd.DataFrame(rows).to_csv(DATA_DIR / f\"votacoes_{tag}__partial.csv\", index=False)\n",
    "            raise\n",
    "        pd.DataFrame(rows).to_csv(arq_mes, index=False)\n",
    "    # consolida (r√°pido e idempotente)\n",
    "    vot_files = sorted(DATA_DIR.glob(\"votacoes_20*.csv\"))\n",
    "    df = pd.concat((pd.read_csv(p) for p in vot_files), ignore_index=True) if vot_files else pd.DataFrame()\n",
    "    df.to_csv(DATA_DIR / \"votacoes.csv\", index=False)\n",
    "    return df\n",
    "\n",
    "def coletar_votos_resumivel(votacoes_df, itens=200):\n",
    "    if votacoes_df.empty:\n",
    "        return pd.DataFrame()\n",
    "    votacoes_df[\"mes_tag\"] = pd.to_datetime(votacoes_df[\"data\"], errors=\"coerce\").dt.strftime(\"%Y-%m\")\n",
    "    sess = make_session()\n",
    "    for tag, grupo in votacoes_df.groupby(\"mes_tag\", dropna=True):\n",
    "        arq_mes = DATA_DIR / f\"votos_{tag}.csv\"\n",
    "        if arq_mes.exists() and arq_mes.stat().st_size > 0:\n",
    "            continue\n",
    "        votos = []\n",
    "        try:\n",
    "            for vid in tqdm(grupo[\"id\"].astype(str), desc=f\"Votos {tag}\"):\n",
    "                pag = 1\n",
    "                while True:\n",
    "                    dados = fetch_page(sess, f\"votacoes/{vid}/votos\", {\"pagina\": pag, \"itens\": itens})\n",
    "                    if not dados: break\n",
    "                    votos.extend(dados)\n",
    "                    if len(dados) < itens: break\n",
    "                    pag += 1\n",
    "                    time.sleep(0.02)\n",
    "        except KeyboardInterrupt:\n",
    "            if votos:\n",
    "                pd.DataFrame(votos).to_csv(DATA_DIR / f\"votos_{tag}__partial.csv\", index=False)\n",
    "            raise\n",
    "        pd.DataFrame(votos).to_csv(arq_mes, index=False)\n",
    "    # consolida\n",
    "    v_files = sorted(DATA_DIR.glob(\"votos_20*.csv\"))\n",
    "    df = pd.concat((pd.read_csv(p) for p in v_files), ignore_index=True) if v_files else pd.DataFrame()\n",
    "    df.to_csv(DATA_DIR / \"votos_deputados.csv\", index=False)\n",
    "    return df\n",
    "\n",
    "# ===== Execu√ß√£o =====\n",
    "votacoes = coletar_votacoes_resumivel(\"2020-01-01\", str(date.today()), itens=200)\n",
    "votos_df = coletar_votos_resumivel(votacoes, itens=200)\n",
    "print(\"OK:\", len(votacoes), \"vota√ß√µes;\", len(votos_df), \"votos.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757fc7b2",
   "metadata": {},
   "source": [
    "# 4. Presen√ßas em Sess√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "62bc0398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK ‚Äî presen√ßas salvas em data/processed/presencas.csv | linhas: 564081\n"
     ]
    }
   ],
   "source": [
    "# Presen√ßas em sess√µes ‚Äî coleta total por ano (independe do DataFrame `eventos`)\n",
    "import io, re, requests, pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import date\n",
    "\n",
    "DATA_DIR = Path(\"data/processed\"); DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT = DATA_DIR / \"presencas.csv\"\n",
    "\n",
    "BASE_ARQ = \"http://dadosabertos.camara.leg.br/arquivos/eventosPresencaDeputados/csv/eventosPresencaDeputados-{ano}.csv\"\n",
    "\n",
    "# Anos a cobrir\n",
    "anos = list(range(2020, date.today().year + 1))\n",
    "\n",
    "def padronizar_presencas(df_raw: pd.DataFrame, ano: int) -> pd.DataFrame:\n",
    "    # normaliza nomes\n",
    "    cols_lower = {c.lower(): c for c in df_raw.columns}\n",
    "\n",
    "    # coluna do id do evento\n",
    "    id_evento_col = None\n",
    "    for key in (\"idevento\",\"id_evento\",\"evento_id\",\"id\"):\n",
    "        if key in cols_lower: id_evento_col = cols_lower[key]; break\n",
    "    if not id_evento_col:\n",
    "        id_evento_col = next((c for c in df_raw.columns if \"evento\" in c.lower()), None)\n",
    "    if not id_evento_col:\n",
    "        raise KeyError(f\"[{ano}] N√£o achei coluna de evento. Colunas: {list(df_raw.columns)}\")\n",
    "\n",
    "    # coluna do id do deputado\n",
    "    id_dep_col = None\n",
    "    for key in (\"iddeputado\",\"id_deputado\",\"idparlamentar\",\"idecadastro\"):\n",
    "        if key in cols_lower: id_dep_col = cols_lower[key]; break\n",
    "    if not id_dep_col:\n",
    "        id_dep_col = next((c for c in df_raw.columns if \"deputad\" in c.lower()), None)\n",
    "\n",
    "    # coluna do tipo de presen√ßa\n",
    "    tipo_col = None\n",
    "    for key in (\"tipopresenca\",\"tipo_presenca\"):\n",
    "        if key in cols_lower: tipo_col = cols_lower[key]; break\n",
    "    if not tipo_col:\n",
    "        tipo_col = next((c for c in df_raw.columns if \"presen\" in c.lower() and \"tipo\" in c.lower()), None)\n",
    "\n",
    "    out = pd.DataFrame()\n",
    "    out[\"id_evento\"] = pd.to_numeric(df_raw[id_evento_col].astype(str).str.extract(r\"(\\d+)\")[0], errors=\"coerce\").astype(\"Int64\")\n",
    "    out[\"id_deputado\"] = (\n",
    "        pd.to_numeric(df_raw[id_dep_col].astype(str).str.extract(r\"(\\d+)\")[0], errors=\"coerce\").astype(\"Int64\")\n",
    "        if id_dep_col else pd.NA\n",
    "    )\n",
    "    out[\"tipo_presenca\"] = df_raw[tipo_col] if tipo_col else pd.NA\n",
    "    out[\"ano_origem\"] = ano\n",
    "    out = out.dropna(subset=[\"id_evento\"])\n",
    "    return out\n",
    "\n",
    "parts = []\n",
    "for ano in anos:\n",
    "    url = BASE_ARQ.format(ano=ano)\n",
    "    r = requests.get(url, timeout=60)\n",
    "    if r.status_code == 404:\n",
    "        # pode n√£o existir para anos sem dados\n",
    "        continue\n",
    "    r.raise_for_status()\n",
    "    buf = io.BytesIO(r.content)\n",
    "    # CSVs do portal costumam usar ';' e encoding UTF-8\n",
    "    df_raw = pd.read_csv(buf, sep=';', dtype=str, low_memory=False)\n",
    "    parts.append(padronizar_presencas(df_raw, ano))\n",
    "\n",
    "presencas = pd.concat(parts, ignore_index=True) if parts else pd.DataFrame(columns=[\"id_evento\",\"id_deputado\",\"tipo_presenca\",\"ano_origem\"])\n",
    "presencas = presencas.drop_duplicates(subset=[\"id_evento\",\"id_deputado\",\"tipo_presenca\"])\n",
    "\n",
    "presencas.to_csv(OUT, index=False)\n",
    "print(f\"OK ‚Äî presen√ßas salvas em {OUT} | linhas: {len(presencas)}\")\n",
    "\n",
    "# Dica: se depois quiser filtrar pelos seus eventos espec√≠ficos, fa√ßa:\n",
    "# eventos = pd.read_csv(DATA_DIR/'eventos_sessoes.csv')  # por ex.\n",
    "# presencas = presencas[presencas['id_evento'].isin(eventos['id_evento'])]\n",
    "# presencas.to_csv(OUT, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d5a80e",
   "metadata": {},
   "source": [
    "# 5. Dados de Remunera√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05b65798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coleta detalhes profissionais\n",
    "remuneracoes = []\n",
    "for id_dep in deputados[\"id\"]:\n",
    "    dados = get_dados(f\"deputados/{id_dep}\")\n",
    "    remuneracoes.append({\n",
    "        \"id_deputado\": id_dep,\n",
    "        \"cargo\": dados.get(\"ultimoStatus\", {}).get(\"condicaoEleitoral\"),\n",
    "        \"situacao\": dados.get(\"ultimoStatus\", {}).get(\"situacao\"),\n",
    "        \"data_inicio\": dados.get(\"ultimoStatus\", {}).get(\"data\"),\n",
    "    })\n",
    "pd.DataFrame(remuneracoes).to_csv(DATA_DIR / \"remuneracoes.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4fbc1d-2d28-47a0-9466-6a902eec703b",
   "metadata": {},
   "source": [
    "### 1Ô∏è‚É£ Deputados:\n",
    "- Geramos **deputados.json** - id, uri, nome, siglaPartido, uriPartido, siglaUF, idLegislatura, urlFoto e email\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "326fe1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coletando lista de deputados...\n",
      "‚úÖ Deputados coletados: 513 ‚Üí ../data/processed/deputados.json\n",
      "‚úÖ Deputados coletados: 513 ‚Üí ../data/processed/deputados.json\n"
     ]
    }
   ],
   "source": [
    "print(\"Coletando lista de deputados...\")\n",
    "deputados = []\n",
    "pagina = 1\n",
    "\n",
    "while True:\n",
    "    params = {\n",
    "        \"itens\": 100,\n",
    "        \"pagina\": pagina,\n",
    "        \"ordem\": \"ASC\",\n",
    "        \"ordenarPor\": \"nome\"\n",
    "    }\n",
    "    \n",
    "    dados = make_request(f\"{BASE_URL}/deputados\", params)\n",
    "    if not dados:\n",
    "        break\n",
    "        \n",
    "    deputados.extend(dados)\n",
    "    pagina += 1\n",
    "    time.sleep(0.1)  # polidez\n",
    "\n",
    "# Salvando dados\n",
    "ARQ_DEPUTADOS.write_text(\n",
    "    json.dumps(deputados, ensure_ascii=False, indent=2),\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Deputados coletados: {len(deputados)} ‚Üí {ARQ_DEPUTADOS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff784d62-1cf9-4e2a-aaa6-23ccbb62472e",
   "metadata": {},
   "source": [
    "### OBS: Frequencia Eventos:\n",
    "- Geramos **freq_eventos.csv** - id_deputado e num_eventos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a3e64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/513 deputados processados‚Ä¶ acumulado eventos = 22793\n",
      "50/513 deputados processados‚Ä¶ acumulado eventos = 43357\n",
      "50/513 deputados processados‚Ä¶ acumulado eventos = 43357\n",
      "75/513 deputados processados‚Ä¶ acumulado eventos = 73656\n",
      "75/513 deputados processados‚Ä¶ acumulado eventos = 73656\n",
      "100/513 deputados processados‚Ä¶ acumulado eventos = 98560\n",
      "100/513 deputados processados‚Ä¶ acumulado eventos = 98560\n",
      "125/513 deputados processados‚Ä¶ acumulado eventos = 119013\n",
      "125/513 deputados processados‚Ä¶ acumulado eventos = 119013\n",
      "150/513 deputados processados‚Ä¶ acumulado eventos = 140355\n",
      "150/513 deputados processados‚Ä¶ acumulado eventos = 140355\n",
      "175/513 deputados processados‚Ä¶ acumulado eventos = 163215\n",
      "175/513 deputados processados‚Ä¶ acumulado eventos = 163215\n",
      "200/513 deputados processados‚Ä¶ acumulado eventos = 186858\n",
      "200/513 deputados processados‚Ä¶ acumulado eventos = 186858\n",
      "225/513 deputados processados‚Ä¶ acumulado eventos = 209412\n",
      "225/513 deputados processados‚Ä¶ acumulado eventos = 209412\n",
      "250/513 deputados processados‚Ä¶ acumulado eventos = 232192\n",
      "250/513 deputados processados‚Ä¶ acumulado eventos = 232192\n",
      "275/513 deputados processados‚Ä¶ acumulado eventos = 255603\n",
      "275/513 deputados processados‚Ä¶ acumulado eventos = 255603\n",
      "300/513 deputados processados‚Ä¶ acumulado eventos = 279524\n",
      "300/513 deputados processados‚Ä¶ acumulado eventos = 279524\n",
      "325/513 deputados processados‚Ä¶ acumulado eventos = 300869\n",
      "325/513 deputados processados‚Ä¶ acumulado eventos = 300869\n",
      "350/513 deputados processados‚Ä¶ acumulado eventos = 322180\n",
      "350/513 deputados processados‚Ä¶ acumulado eventos = 322180\n",
      "375/513 deputados processados‚Ä¶ acumulado eventos = 343063\n",
      "375/513 deputados processados‚Ä¶ acumulado eventos = 343063\n",
      "400/513 deputados processados‚Ä¶ acumulado eventos = 365955\n",
      "400/513 deputados processados‚Ä¶ acumulado eventos = 365955\n",
      "425/513 deputados processados‚Ä¶ acumulado eventos = 388968\n",
      "425/513 deputados processados‚Ä¶ acumulado eventos = 388968\n",
      "450/513 deputados processados‚Ä¶ acumulado eventos = 405999\n",
      "450/513 deputados processados‚Ä¶ acumulado eventos = 405999\n",
      "475/513 deputados processados‚Ä¶ acumulado eventos = 429793\n",
      "475/513 deputados processados‚Ä¶ acumulado eventos = 429793\n",
      "500/513 deputados processados‚Ä¶ acumulado eventos = 453913\n",
      "500/513 deputados processados‚Ä¶ acumulado eventos = 453913\n",
      "513/513 deputados processados‚Ä¶ acumulado eventos = 464351\n",
      "‚úÖ Contagens geradas ‚Üí ../data/processed/freq_eventos.csv\n",
      "üìä Total estimado de eventos (2020‚Üíhoje): 464351\n",
      "513/513 deputados processados‚Ä¶ acumulado eventos = 464351\n",
      "‚úÖ Contagens geradas ‚Üí ../data/processed/freq_eventos.csv\n",
      "üìä Total estimado de eventos (2020‚Üíhoje): 464351\n"
     ]
    }
   ],
   "source": [
    "# Carregar IDs dos deputados\n",
    "with open(ARQ_DEPUTADOS, \"r\", encoding=\"utf-8\") as f:\n",
    "    ids_deputados = [int(d[\"id\"]) for d in json.load(f)]\n",
    "\n",
    "# Preparar CSV de sa√≠da\n",
    "with open(ARQ_FREQ, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    csv.writer(f).writerow([\"id_deputado\", \"num_eventos\"])\n",
    "\n",
    "print(\"Coletando eventos por deputado...\")\n",
    "total_eventos = 0\n",
    "\n",
    "for i, dep_id in enumerate(ids_deputados, 1):\n",
    "    total_dep = 0\n",
    "    pagina = 1\n",
    "    \n",
    "    while True:\n",
    "        params = {\n",
    "            \"dataInicio\": DATA_INICIO,\n",
    "            \"dataFim\": DATA_FIM,\n",
    "            \"itens\": 100,\n",
    "            \"ordenarPor\": \"dataHoraInicio\",\n",
    "            \"ordem\": \"ASC\",\n",
    "            \"pagina\": pagina\n",
    "        }\n",
    "        \n",
    "        dados = make_request(f\"{BASE_URL}/deputados/{dep_id}/eventos\", params)\n",
    "        if not dados:\n",
    "            break\n",
    "            \n",
    "        total_dep += len(dados)\n",
    "        pagina += 1\n",
    "        time.sleep(0.1)  # polidez\n",
    "    \n",
    "    # Gravar contagem do deputado\n",
    "    with open(ARQ_FREQ, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        csv.writer(f).writerow([dep_id, total_dep])\n",
    "    \n",
    "    total_eventos += total_dep\n",
    "    \n",
    "    # Log de progresso\n",
    "    if i % 25 == 0 or i == len(ids_deputados):\n",
    "        print(f\"{i}/{len(ids_deputados)} deputados processados... Total eventos = {total_eventos}\")\n",
    "\n",
    "print(f\"‚úÖ Contagens geradas ‚Üí {ARQ_FREQ}\")\n",
    "print(f\"üìä Total de eventos coletados: {total_eventos}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc34cfe",
   "metadata": {},
   "source": [
    "## 4 Presen√ßas em sess√µes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5639f9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK ‚Äî presen√ßas salvas em data/processed/presencas.csv | linhas: 564081\n"
     ]
    }
   ],
   "source": [
    "# Presen√ßas em sess√µes ‚Äî coleta total por ano (independe do DataFrame `eventos`)\n",
    "import io, re, requests, pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import date\n",
    "\n",
    "DATA_DIR = Path(\"data/processed\"); DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT = DATA_DIR / \"presencas.csv\"\n",
    "\n",
    "BASE_ARQ = \"http://dadosabertos.camara.leg.br/arquivos/eventosPresencaDeputados/csv/eventosPresencaDeputados-{ano}.csv\"\n",
    "\n",
    "# Anos a cobrir\n",
    "anos = list(range(2020, date.today().year + 1))\n",
    "\n",
    "def padronizar_presencas(df_raw: pd.DataFrame, ano: int) -> pd.DataFrame:\n",
    "    # normaliza nomes\n",
    "    cols_lower = {c.lower(): c for c in df_raw.columns}\n",
    "\n",
    "    # coluna do id do evento\n",
    "    id_evento_col = None\n",
    "    for key in (\"idevento\",\"id_evento\",\"evento_id\",\"id\"):\n",
    "        if key in cols_lower: id_evento_col = cols_lower[key]; break\n",
    "    if not id_evento_col:\n",
    "        id_evento_col = next((c for c in df_raw.columns if \"evento\" in c.lower()), None)\n",
    "    if not id_evento_col:\n",
    "        raise KeyError(f\"[{ano}] N√£o achei coluna de evento. Colunas: {list(df_raw.columns)}\")\n",
    "\n",
    "    # coluna do id do deputado\n",
    "    id_dep_col = None\n",
    "    for key in (\"iddeputado\",\"id_deputado\",\"idparlamentar\",\"idecadastro\"):\n",
    "        if key in cols_lower: id_dep_col = cols_lower[key]; break\n",
    "    if not id_dep_col:\n",
    "        id_dep_col = next((c for c in df_raw.columns if \"deputad\" in c.lower()), None)\n",
    "\n",
    "    # coluna do tipo de presen√ßa\n",
    "    tipo_col = None\n",
    "    for key in (\"tipopresenca\",\"tipo_presenca\"):\n",
    "        if key in cols_lower: tipo_col = cols_lower[key]; break\n",
    "    if not tipo_col:\n",
    "        tipo_col = next((c for c in df_raw.columns if \"presen\" in c.lower() and \"tipo\" in c.lower()), None)\n",
    "\n",
    "    out = pd.DataFrame()\n",
    "    out[\"id_evento\"] = pd.to_numeric(df_raw[id_evento_col].astype(str).str.extract(r\"(\\d+)\")[0], errors=\"coerce\").astype(\"Int64\")\n",
    "    out[\"id_deputado\"] = (\n",
    "        pd.to_numeric(df_raw[id_dep_col].astype(str).str.extract(r\"(\\d+)\")[0], errors=\"coerce\").astype(\"Int64\")\n",
    "        if id_dep_col else pd.NA\n",
    "    )\n",
    "    out[\"tipo_presenca\"] = df_raw[tipo_col] if tipo_col else pd.NA\n",
    "    out[\"ano_origem\"] = ano\n",
    "    out = out.dropna(subset=[\"id_evento\"])\n",
    "    return out\n",
    "\n",
    "parts = []\n",
    "for ano in anos:\n",
    "    url = BASE_ARQ.format(ano=ano)\n",
    "    r = requests.get(url, timeout=60)\n",
    "    if r.status_code == 404:\n",
    "        # pode n√£o existir para anos sem dados\n",
    "        continue\n",
    "    r.raise_for_status()\n",
    "    buf = io.BytesIO(r.content)\n",
    "    # CSVs do portal costumam usar ';' e encoding UTF-8\n",
    "    df_raw = pd.read_csv(buf, sep=';', dtype=str, low_memory=False)\n",
    "    parts.append(padronizar_presencas(df_raw, ano))\n",
    "\n",
    "presencas = pd.concat(parts, ignore_index=True) if parts else pd.DataFrame(columns=[\"id_evento\",\"id_deputado\",\"tipo_presenca\",\"ano_origem\"])\n",
    "presencas = presencas.drop_duplicates(subset=[\"id_evento\",\"id_deputado\",\"tipo_presenca\"])\n",
    "\n",
    "presencas.to_csv(OUT, index=False)\n",
    "print(f\"OK ‚Äî presen√ßas salvas em {OUT} | linhas: {len(presencas)}\")\n",
    "\n",
    "# Dica: se depois quiser filtrar pelos seus eventos espec√≠ficos, fa√ßa:\n",
    "# eventos = pd.read_csv(DATA_DIR/'eventos_sessoes.csv')  # por ex.\n",
    "# presencas = presencas[presencas['id_evento'].isin(eventos['id_evento'])]\n",
    "# presencas.to_csv(OUT, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ab61ac",
   "metadata": {},
   "source": [
    "### Situa√ß√£o parlamentar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"cod\": \"\",\n",
      "    \"sigla\": \"A\",\n",
      "    \"nome\": \"Afastado\",\n",
      "    \"descricao\": \"\"\n",
      "  },\n",
      "  {\n",
      "    \"cod\": \"\",\n",
      "    \"sigla\": \"C\",\n",
      "    \"nome\": \"Convocado\",\n",
      "    \"descricao\": \"\"\n",
      "  },\n",
      "  {\n",
      "    \"cod\": \"\",\n",
      "    \"sigla\": \"E\",\n",
      "    \"nome\": \"Exerc√≠cio\",\n",
      "    \"descricao\": \"\"\n",
      "  },\n",
      "  {\n",
      "    \"cod\": \"\",\n",
      "    \"sigla\": \"F\",\n",
      "    \"nome\": \"Fim de Mandato\",\n",
      "    \"descricao\": \"\"\n",
      "  },\n",
      "  {\n",
      "    \"cod\": \"\",\n",
      "    \"sigla\": \"L\",\n",
      "    \"nome\": \"Licen√ßa\",\n",
      "    \"descricao\": \"\"\n",
      "  },\n",
      "  {\n",
      "    \"cod\": \"\",\n",
      "    \"sigla\": \"S\",\n",
      "    \"nome\": \"Supl√™ncia\",\n",
      "    \"descricao\": \"\"\n",
      "  },\n",
      "  {\n",
      "    \"cod\": \"\",\n",
      "    \"sigla\": \"U\",\n",
      "    \"nome\": \"Suspenso\",\n",
      "    \"descricao\": \"\"\n",
      "  },\n",
      "  {\n",
      "    \"cod\": \"\",\n",
      "    \"sigla\": \"V\",\n",
      "    \"nome\": \"Vac√¢ncia\",\n",
      "    \"descricao\": \"\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import os, time, json, csv, requests\n",
    "from datetime import date\n",
    "\n",
    "BASE_URL = \"https://dadosabertos.camara.leg.br/api/v2\"\n",
    "DIR_SAIDA = \"../data/processed\"\n",
    "ARQ_DEPUTADOS = f\"{DIR_SAIDA}/deputados.json\"\n",
    "ARQ_FREQ = f\"{DIR_SAIDA}/freq_eventos.csv\"\n",
    "\n",
    "os.makedirs(DIR_SAIDA, exist_ok=True)\n",
    "\n",
    "# janela temporal\n",
    "DATA_INICIO = \"2020-01-01\"\n",
    "DATA_FIM = str(date.today())  # hoje\n",
    "\n",
    "# sess√£o HTTP enxuta\n",
    "sessao = requests.Session()\n",
    "sessao.headers.update({\n",
    "    \"Accept\": \"application/json\",\n",
    "    \"User-Agent\": \"PUCRS-Assiduidade/1.0 (contato: seu_email@exemplo.com)\"\n",
    "})\n",
    "\n",
    "\n",
    "r = sessao.get(f\"{BASE_URL}/referencias/deputados/codSituacao\")\n",
    "situacoes = r.json()[\"dados\"]\n",
    "print(json.dumps(situacoes, indent=2, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e6258c",
   "metadata": {},
   "source": [
    "### Ocupa√ß√µes por ID\n",
    "- Geramos **ocupacoes.csv** - id_deputado, titulo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b471d11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Coletando ocupa√ß√µes dos deputados...\")\n",
    "# Preparar CSV de ocupa√ß√µes\n",
    "with open(ARQ_OCUPACOES, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    csv.writer(f).writerow([\"id_deputado\", \"titulo\", \"dataInicio\", \"dataFim\"])\n",
    "\n",
    "# Carregar IDs dos deputados\n",
    "with open(ARQ_DEPUTADOS, \"r\", encoding=\"utf-8\") as f:\n",
    "    ids_deputados = [int(d[\"id\"]) for d in json.load(f)]\n",
    "\n",
    "total_ocupacoes = 0\n",
    "for i, dep_id in enumerate(ids_deputados, 1):\n",
    "    dados = make_request(f\"{BASE_URL}/deputados/{dep_id}/ocupacoes\")\n",
    "    if dados:\n",
    "        with open(ARQ_OCUPACOES, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            w = csv.writer(f)\n",
    "            for o in dados:\n",
    "                w.writerow([\n",
    "                    dep_id,\n",
    "                    o.get(\"titulo\"),\n",
    "                    o.get(\"dataInicio\"),\n",
    "                    o.get(\"dataFim\")\n",
    "                ])\n",
    "                total_ocupacoes += 1\n",
    "    \n",
    "    if i % 25 == 0:\n",
    "        print(f\"... {i}/{len(ids_deputados)} deputados processados\")\n",
    "    time.sleep(0.1)\n",
    "\n",
    "print(f\"‚úÖ Ocupa√ß√µes coletadas: {total_ocupacoes} ‚Üí {ARQ_OCUPACOES}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tde",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
